{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMbVkf0CftovZPGEoT41eo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codekikicode/python-coding-folio/blob/main/Project2_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXZ6cbxM6_DP",
        "outputId": "5c2adad7-650e-4737-8c56-e46cb915387d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch, time, math, os\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.backends.cudnn.benchmark = True  # autotune kernels when shapes are stable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Tiny Shakespeare dataset"
      ],
      "metadata": {
        "id": "PSnAZI598DjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, pathlib\n",
        "pathlib.Path(\"input.txt\").parent.mkdir(parents=True, exist_ok=True)\n",
        "if not pathlib.Path(\"input.txt\").exists():\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\",\n",
        "        \"input.txt\"\n",
        "    )\n",
        "print(\"Dataset ready:\", os.path.getsize(\"input.txt\"), \"bytes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu-RDiTA8AVX",
        "outputId": "b5e61ade-6e11-42e3-ded5-66e8aa53dc7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ready: 1115394 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess data | Split Training and Validation Data"
      ],
      "metadata": {
        "id": "Hd_P0GSv8Rax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"input.txt\",\"r\",encoding=\"utf-8\").read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data, val_data = data[:n], data[n:]\n",
        "print(\"Splits:\", len(train_data), len(val_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-l2Yaiw8PC0",
        "outputId": "305e5bc0-d685-4b39-af4e-78459ec42818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 65\n",
            "Splits: 1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Hyperparameters"
      ],
      "metadata": {
        "id": "yDQpgLHn8blH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "batch_size    = 64\n",
        "block_size    = 256\n",
        "learning_rate = 3e-4\n",
        "max_iters     = 5000\n",
        "eval_interval = 1000\n",
        "eval_iters    = 25\n",
        "\n",
        "# Model scale\n",
        "n_embd, n_head, n_layer, dropout = 384, 6, 6, 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqgWasnP8aqq",
        "outputId": "d06ea2fa-944d-4891-a31c-a046b2386337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d3198fefdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    d = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size]       for i in ix]).to(device)\n",
        "    y = torch.stack([d[i+1:i+block_size+1]  for i in ix]).to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "-7upQxWe8nxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer model with Dropout ON"
      ],
      "metadata": {
        "id": "PyPdYQiP8yA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k, q = self.key(x), self.query(x)\n",
        "        wei = (q @ k.transpose(-2, -1)) * (C ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj  = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ff = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb   = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks    = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f      = nn.LayerNorm(n_embd)\n",
        "        self.lm_head   = nn.Linear(n_embd, vocab_size)\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok = self.token_emb(idx)\n",
        "        pos = self.pos_emb(torch.arange(T, device=idx.device))\n",
        "        x = tok + pos\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits = logits.view(B*T, -1)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "jXgWrc6X8y2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instatiate | Train data (Timed)"
      ],
      "metadata": {
        "id": "Ubf9xugS87uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, math, torch\n",
        "\n",
        "# Instantiate\n",
        "model = GPTLanguageModel().to(device)\n",
        "\n",
        "#Boosts compiling speed\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "    print(\"Compiled with torch.compile\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scaler = torch.amp.GradScaler(device=\"cuda\" if device == \"cuda\" else None)\n",
        "\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train','val']:\n",
        "            s = 0.0\n",
        "            for _ in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                _, l = model(X, Y)\n",
        "                s += l.item()\n",
        "            out[split] = s / eval_iters\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "start = time.time()\n",
        "print(\"Training started at:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(start)))\n",
        "\n",
        "for it in range(max_iters):\n",
        "    if it % eval_interval == 0 or it == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {it}: train {losses['train']:.4f}, val {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    with torch.amp.autocast(device_type=\"cuda\" if device == \"cuda\" else \"cpu\", enabled=(device=='cuda')):\n",
        "        _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    if device == 'cuda' and scaler is not None:\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    else:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "end = time.time()\n",
        "print(\"Training finished at:\", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(end)))\n",
        "print(f\"Total training time: {end - start:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peaO9uSZBN9J",
        "outputId": "f65440a3-2e23-4cc5-e0c0-8f213046c961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiled with torch.compile\n",
            "Training started at: 2025-08-11 03:01:12\n",
            "step 0: train 4.2702, val 4.2688\n",
            "step 1000: train 1.6208, val 1.7927\n",
            "step 2000: train 1.3509, val 1.5676\n",
            "step 3000: train 1.2419, val 1.4974\n",
            "step 4000: train 1.1491, val 1.4702\n",
            "step 4999: train 1.0798, val 1.4731\n",
            "Training finished at: 2025-08-11 03:13:11\n",
            "Total training time: 718.86 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate 5000 characters | Report metrics"
      ],
      "metadata": {
        "id": "EKAj0gxSCkh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 5k characters\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    ctx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "    out_tokens = model.generate(ctx, max_new_tokens=5000)[0].tolist()\n",
        "sample = decode(out_tokens)\n",
        "\n",
        "print(sample[:1000])               # preview\n",
        "print(\"\\n--- total length:\", len(sample))\n",
        "\n",
        "# Final loss + perplexity\n",
        "def final_metrics():\n",
        "    model.eval()\n",
        "    s = 0.0\n",
        "    with torch.inference_mode():\n",
        "        for _ in range(eval_iters):\n",
        "            X, Y = get_batch('val')\n",
        "            _, l = model(X, Y)\n",
        "            s += l.item()\n",
        "    vloss = s / eval_iters\n",
        "    return vloss, math.exp(vloss)\n",
        "\n",
        "vloss, ppl = final_metrics()\n",
        "print(f\"Final val loss ~ {vloss:.4f} | perplexity ~ {ppl:.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MH4P4F_CihR",
        "outputId": "be17165c-5a6e-4316-acad-9876600f015c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Condition!\n",
            "Welcome, damnned slain, like venom'd; there's\n",
            "That call ic to Rome, take not the business.\n",
            "\n",
            "KING LEWIS XI:\n",
            "What they say 'tis choice of laque\n",
            "And gian false your breasts blood. A wige, my lords,\n",
            "Have I to them. Come, lady, Barnardine,\n",
            "To be me the rest, Lucentio, be I am:\n",
            "But set another gentlemen; since I have not spoke\n",
            "At Duke of Angelo seem to his fastime.\n",
            "\n",
            "ROMEO:\n",
            "Now Romeo, Marcius,\n",
            "Richard Romeo, show more than his abund himself:\n",
            "No money, what is valiant for it be?\n",
            "\n",
            "JULIET:\n",
            "O great Ratnand!\n",
            "\n",
            "ROMEO:\n",
            "I farewell, go-day.\n",
            "\n",
            "JULIET:\n",
            "And Duy ne! why, hhath baggarly bed to hear speak.\n",
            "\n",
            "ROMEO:\n",
            "Darry he?\n",
            "\n",
            "JULIET:\n",
            "Madam, with the glory pities, report your foot,\n",
            "drawn by the unwhether queen had sunguish'd up in him:\n",
            "Here, Margaret, never known in a breath:\n",
            "'Tis marriage by such of the envious beauty\n",
            "That which they shall be coal. Harket you know'st! Thou,\n",
            "What should he had been forget a parlous burden leave,\n",
            "mightily shores forth be a life, this whether,\n",
            "No less was straight beg\n",
            "\n",
            "--- total length: 5001\n",
            "Final val loss ~ 1.4772 | perplexity ~ 4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Anaysis: The model achieved a final validation loss of approximately 1.4772 and a perplexity of about 4.4, which is a notable improvement over the baseline. Lower loss and perplexity indicate the model is more confident in its next-character predictions. While occasional nonsensical phrases still appear, the output consistently preserves play-like structure (speaker labels, short verse lines) and maintains Shakespearean-style vocabulary, suggesting effective learning.\n",
        "\n",
        "Increasing embedding size to 384 dimensions with 6 heads Ã— 6 layers and adding dropout = 0.2 improved generalization and reduced overfitting, as reflected in both the cleaner text generation and the drop in validation loss.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RCdqn2g_F4_D"
      }
    }
  ]
}